{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9293b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed496df",
   "metadata": {},
   "source": [
    "## SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ace126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, name: str = \"SwiGLU\"):\n",
    "        super(SwiGLU, self).__init__()\n",
    "\n",
    "        self.name = name\n",
    "        self.constant = 0.044715\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "\n",
    "        swish = x * torch.sigmoid(x)\n",
    "        gelu = 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / math.pi)) * (x + self.constant * torch.pow(x, 3))))\n",
    "        return swish * gelu\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    activation_func = SwiGLU()\n",
    "    \n",
    "    texts = torch.randn((64, 128, 512))\n",
    "\n",
    "    assert (activation_func(texts).size()) == (64, 128, 512), \"SwiGLU activation function is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ea9b3",
   "metadata": {},
   "source": [
    "## RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormalization(nn.Module):\n",
    "    def __init__(self, dimension: int = 512, eps: float = 1e-4):\n",
    "        super(RMSNormalization, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = nn.Parameter(\n",
    "            data=torch.ones(\n",
    "                (\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension,\n",
    "                )\n",
    "            ),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "        RMS = torch.sqrt(torch.mean(x**2, dim=-1) + self.eps)\n",
    "        RMS = RMS.unsqueeze(dim=-1)\n",
    "\n",
    "        RMSNorm = x / RMS\n",
    "\n",
    "        return torch.mul(RMSNorm, self.gamma)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    norm = RMSNormalization(dimension=512)\n",
    "\n",
    "    assert (norm(torch.randn(64, 128, 512)).size()) == (\n",
    "        64,\n",
    "        128,\n",
    "        512,\n",
    "    ), \"RMSNormalization is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e7146",
   "metadata": {},
   "source": [
    "## RoPE - Rotary Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dimension: int = 512, sequence_length: int = 128, base: int = 10000):\n",
    "        super(RoPE, self).__init__()\n",
    "        \n",
    "        self.dimension = dimension//2\n",
    "        self.sequence_length = sequence_length\n",
    "        self.base = base\n",
    "        \n",
    "        self.sin_values = torch.zeros((self.sequence_length, self.dimension))\n",
    "        self.cos_values = torch.zeros((self.sequence_length, self.dimension))\n",
    "        \n",
    "        for position in range(self.sequence_length):\n",
    "            for i in range(self.dimension):\n",
    "                inverse_frequncy = 1.0 / (self.base ** (2 * (i // 2) / self.dimension))\n",
    "                \n",
    "                theta = position * inverse_frequncy\n",
    "                \n",
    "                self.sin_values[position, i] = math.sin(theta)\n",
    "                self.cos_values[position, i] = math.cos(theta)\n",
    "                \n",
    "        self.register_buffer(\"sin\", self.sin_values.unsqueeze(dim=0))\n",
    "        self.register_buffer(\"cos\", self.cos_values.unsqueeze(dim=0))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "        \n",
    "        x1 = x[..., 0::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        \n",
    "        sin = self.sin[:, :x.size(1), :]\n",
    "        cos = self.cos[:, :x.size(1), :]\n",
    "\n",
    "        rotated_even = x1 * cos - x2 * sin\n",
    "        rotated_odd  = x1 * sin + x2 * cos\n",
    "\n",
    "        \n",
    "        output = torch.stack((rotated_even, rotated_odd), dim=-1)\n",
    "        output = output.view(output.size(0), output.size(1), -1)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    encoding = RoPE(dimension=512, sequence_length=128)\n",
    "    \n",
    "    texts = torch.randn((64, 128, 512))\n",
    "    \n",
    "    assert (encoding(texts).size()) == (64, 128, 512), \"RoPE is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba1d74e",
   "metadata": {},
   "source": [
    "## Grouped Query Attention Layer - GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, dimension: int = 512, query_heads: int = 8, kv_heads: int = 4):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.query_heads = query_heads\n",
    "        self.kv_heads = kv_heads\n",
    "\n",
    "        assert (\n",
    "            self.dimension % self.query_heads == 0\n",
    "        ), \"Dimension must be divisible by query heads\".capitalize()\n",
    "        assert (\n",
    "            self.dimension % self.kv_heads == 0\n",
    "        ), \"Dimension must be divisible by kv heads\".capitalize()\n",
    "\n",
    "        self.head_dim = self.dimension // self.query_heads\n",
    "        self.num_of_repeatation = self.query_heads // self.kv_heads\n",
    "\n",
    "        self.query = nn.Linear(\n",
    "            in_features=self.dimension,\n",
    "            out_features=self.query_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.key = nn.Linear(\n",
    "            in_features=self.dimension,\n",
    "            out_features=self.kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.value = nn.Linear(\n",
    "            in_features=self.dimension,\n",
    "            out_features=self.kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            in_features=self.dimension, out_features=self.dimension, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        assert (\n",
    "            key.size() == value.size()\n",
    "        ), \"Key and value must have the same size\".capitalize()\n",
    "\n",
    "        query = query.view(\n",
    "            query.size(0),\n",
    "            query.size(1),\n",
    "            self.query_heads,\n",
    "            query.size(-1) // self.query_heads,\n",
    "        )\n",
    "        key = key.view(\n",
    "            key.size(0), key.size(1), self.kv_heads, key.size(-1) // self.kv_heads\n",
    "        )\n",
    "        value = value.view(\n",
    "            value.size(0), value.size(1), self.kv_heads, value.size(-1) // self.kv_heads\n",
    "        )\n",
    "\n",
    "        query = query.permute(0, 2, 1, 3)\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "        key = torch.repeat_interleave(input=key, repeats=self.num_of_repeatation, dim=1)\n",
    "        value = torch.repeat_interleave(\n",
    "            input=value, repeats=self.num_of_repeatation, dim=1\n",
    "        )\n",
    "\n",
    "        attention = torch.matmul(\n",
    "            query, torch.transpose(input=key, dim0=-1, dim1=-2)\n",
    "        ) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "        attention = torch.softmax(input=attention, dim=-1)\n",
    "\n",
    "        attention = torch.matmul(input=attention, other=value)\n",
    "        attention = torch.permute(input=attention, dims=(0, 2, 1, 3))\n",
    "\n",
    "        attention = attention.reshape(\n",
    "            attention.size(0), attention.size(1), attention.size(2) * attention.size(3)\n",
    "        )\n",
    "\n",
    "        attention = self.output(attention)\n",
    "\n",
    "        return attention\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attention = GroupedQueryAttention()\n",
    "    texts = torch.randn((64, 128, 512))\n",
    "\n",
    "    output = attention(texts)\n",
    "\n",
    "    assert (\n",
    "        output.size() == (64, 128, 512)\n",
    "    ), \"GroupedQueryAttention is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86e67a",
   "metadata": {},
   "source": [
    "## MLP - Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f20f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dimension: int = 4096,\n",
    "        output_dimension: int = 14336,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        self.bias = bias\n",
    "\n",
    "        self.gate_projection = nn.Linear(\n",
    "            in_features=self.hidden_dimension,\n",
    "            out_features=self.output_dimension,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "        self.up_projection = nn.Linear(\n",
    "            in_features=self.hidden_dimension,\n",
    "            out_features=self.output_dimension,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "        self.down_projection = nn.Linear(\n",
    "            in_features=self.output_dimension,\n",
    "            out_features=self.hidden_dimension,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.swish = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "\n",
    "        gate_output = self.gate_projection(x)\n",
    "        up_output = self.up_projection(x)\n",
    "        up_output = self.swish(up_output)\n",
    "\n",
    "        activation = torch.mul(input=gate_output, other=up_output)\n",
    "\n",
    "        return self.down_projection(activation)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    network = FeedForwardNeuralNetwork(hidden_dimension=512, output_dimension=4 * 512)\n",
    "    texts = torch.randn((64, 128, 512))\n",
    "    print(network(texts).size())\n",
    "\n",
    "    assert (\n",
    "        network(texts).size() == (64, 128, 512)\n",
    "    ), \"FeedForwardNeuralNetwork is not working properly\".capitalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
