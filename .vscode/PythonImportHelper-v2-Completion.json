[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "draw_graph",
        "importPath": "torchview",
        "description": "torchview",
        "isExtraImport": true,
        "detail": "torchview",
        "documentation": {}
    },
    {
        "label": "draw_graph",
        "importPath": "torchview",
        "description": "torchview",
        "isExtraImport": true,
        "detail": "torchview",
        "documentation": {}
    },
    {
        "label": "draw_graph",
        "importPath": "torchview",
        "description": "torchview",
        "isExtraImport": true,
        "detail": "torchview",
        "documentation": {}
    },
    {
        "label": "RoPE",
        "importPath": "positional_encoding",
        "description": "positional_encoding",
        "isExtraImport": true,
        "detail": "positional_encoding",
        "documentation": {}
    },
    {
        "label": "RoPE",
        "importPath": "positional_encoding",
        "description": "positional_encoding",
        "isExtraImport": true,
        "detail": "positional_encoding",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "rms_norm",
        "description": "rms_norm",
        "isExtraImport": true,
        "detail": "rms_norm",
        "documentation": {}
    },
    {
        "label": "SwiGLU",
        "importPath": "activation_func",
        "description": "activation_func",
        "isExtraImport": true,
        "detail": "activation_func",
        "documentation": {}
    },
    {
        "label": "GroupedQueryAttention",
        "importPath": "attention",
        "description": "attention",
        "isExtraImport": true,
        "detail": "attention",
        "documentation": {}
    },
    {
        "label": "FeedForwardNeuralNetwork",
        "importPath": "feedforward",
        "description": "feedforward",
        "isExtraImport": true,
        "detail": "feedforward",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "SwiGLU",
        "kind": 6,
        "importPath": "src.activation_func",
        "description": "src.activation_func",
        "peekOfCode": "class SwiGLU(nn.Module):\n    def __init__(self, name: str = \"SwiGLU\"):\n        super(SwiGLU, self).__init__()\n        self.name = name\n        self.constant = 0.044715\n    def forward(self, x: torch.Tensor):\n        if not isinstance(x, torch.Tensor):\n            raise TypeError(\"Input must be a torch.Tensor\")\n        swish = x * torch.sigmoid(x)\n        gelu = 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / math.pi)) * (x + self.constant * torch.pow(x, 3))))",
        "detail": "src.activation_func",
        "documentation": {}
    },
    {
        "label": "GroupedQueryAttention",
        "kind": 6,
        "importPath": "src.attention",
        "description": "src.attention",
        "peekOfCode": "class GroupedQueryAttention(nn.Module):\n    def __init__(self, dimension: int = 512, query_heads: int = 8, kv_heads: int = 4, sequence_length: int = 128):\n        super(GroupedQueryAttention, self).__init__()\n        self.dimension = dimension\n        self.query_heads = query_heads\n        self.kv_heads = kv_heads\n        self.sequence_length = sequence_length\n        warnings.warn(\n            \"\"\"If you are defined the query heads = 8 and kv heads = 4, it is recommended to use the SwiGLU activation function\"\"\",\n            UserWarning,",
        "detail": "src.attention",
        "documentation": {}
    },
    {
        "label": "FeedForwardNeuralNetwork",
        "kind": 6,
        "importPath": "src.feedforward",
        "description": "src.feedforward",
        "peekOfCode": "class FeedForwardNeuralNetwork(nn.Module):\n    def __init__(\n        self,\n        hidden_dimension: int = 4096,\n        output_dimension: int = 14336,\n        bias: bool = True,\n    ):\n        super(FeedForwardNeuralNetwork, self).__init__()\n        self.hidden_dimension = hidden_dimension\n        self.output_dimension = output_dimension",
        "detail": "src.feedforward",
        "documentation": {}
    },
    {
        "label": "RoPE",
        "kind": 6,
        "importPath": "src.positional_encoding",
        "description": "src.positional_encoding",
        "peekOfCode": "class RoPE(nn.Module):\n    def __init__(\n        self, dimension: int = 512, sequence_length: int = 128, base: int = 10000\n    ):\n        super(RoPE, self).__init__()\n        self.dimension = dimension // 2\n        self.sequence_length = sequence_length\n        self.base = base\n        self.sin_values = torch.zeros((self.sequence_length, self.dimension))\n        self.cos_values = torch.zeros((self.sequence_length, self.dimension))",
        "detail": "src.positional_encoding",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "src.rms_norm",
        "description": "src.rms_norm",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, dimension: int = 512, eps: float = 1e-4):\n        super(RMSNorm, self).__init__()\n        self.dimension = dimension\n        self.eps = eps\n        self.gamma = nn.Parameter(\n            data=torch.ones((1, 1, self.dimension)), requires_grad=True\n        )\n    def forward(self, x: torch.Tensor):\n        if not isinstance(x, torch.Tensor):",
        "detail": "src.rms_norm",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "src.transformer_block",
        "description": "src.transformer_block",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dimension: int = 512,\n        query_heads: int = 8,\n        kv_heads: int = 4,\n        eps: float = 1e-4,\n        sequence_length: int = 128,\n        base: int = 10000,\n        output_dimension: int = 14336,",
        "detail": "src.transformer_block",
        "documentation": {}
    },
    {
        "label": "UnitTest",
        "kind": 6,
        "importPath": "unittest.test",
        "description": "unittest.test",
        "peekOfCode": "class UnitTest(unittest.TestCase):\n    def setUp(self):\n        self.batch_size = 64\n        self.sequence_length = 128\n        self.dimension_size = 512\n        self.query_heads = 8\n        self.kv_heads = 4\n        self.activation_func = SwiGLU()\n        self.rms_normalization = RMSNorm(dimension=self.dimension_size)\n        self.attention = GroupedQueryAttention(",
        "detail": "unittest.test",
        "documentation": {}
    }
]