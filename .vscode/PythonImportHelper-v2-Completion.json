[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "importPath": "rms_norm",
        "description": "rms_norm",
        "isExtraImport": true,
        "detail": "rms_norm",
        "documentation": {}
    },
    {
        "label": "SwiGLU",
        "importPath": "activation_func",
        "description": "activation_func",
        "isExtraImport": true,
        "detail": "activation_func",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "SwiGLU",
        "kind": 6,
        "importPath": "src.activation_func",
        "description": "src.activation_func",
        "peekOfCode": "class SwiGLU(nn.Module):\n    def __init__(self, name: str = \"SwiGLU\"):\n        super(SwiGLU, self).__init__()\n        self.name = name\n        self.constant = 0.044715\n    def forward(self, x: torch.Tensor):\n        if not isinstance(x, torch.Tensor):\n            raise TypeError(\"Input must be a torch.Tensor\")\n        swish = x * torch.sigmoid(x)\n        gelu = 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / math.pi)) * (x + self.constant * torch.pow(x, 3))))",
        "detail": "src.activation_func",
        "documentation": {}
    },
    {
        "label": "GroupedQueryAttention",
        "kind": 6,
        "importPath": "src.attention",
        "description": "src.attention",
        "peekOfCode": "class GroupedQueryAttention(nn.Module):\n    def __init__(self, dimension: int = 512, query_heads: int = 8, kv_heads: int = 4):\n        super(GroupedQueryAttention, self).__init__()\n        self.dimension = dimension\n        self.query_heads = query_heads\n        self.kv_heads = kv_heads\n        assert self.dimension % self.query_heads == 0, \"Dimension must be divisible by query heads\".capitalize()\n        assert self.dimension % self.kv_heads == 0, \"Dimension must be divisible by kv heads\".capitalize()\n    def forward(self, x: torch.Tensor):\n        if not isinstance(x, torch.Tensor):",
        "detail": "src.attention",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "src.rms_norm",
        "description": "src.rms_norm",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, dimension: int = 512, eps: float = 1e-4):\n        super(RMSNorm, self).__init__()\n        self.dimension = dimension\n        self.eps = eps\n        self.gamma = nn.Parameter(\n            data=torch.ones((1, 1, self.dimension)), requires_grad=True\n        )\n    def forward(self, x: torch.Tensor):\n        if not isinstance(x, torch.Tensor):",
        "detail": "src.rms_norm",
        "documentation": {}
    },
    {
        "label": "UnitTest",
        "kind": 6,
        "importPath": "unittest.test",
        "description": "unittest.test",
        "peekOfCode": "class UnitTest(unittest.TestCase):\n    def setUp(self):\n        self.batch_size = 64\n        self.sequence_length = 128\n        self.dimension_size = 512\n        self.activation_func = SwiGLU()\n        self.rms_normalization = RMSNorm(dimension=self.dimension_size)\n    def test_activation_func(self):\n        texts = torch.randn(\n            (self.batch_size, self.sequence_length, self.dimension_size)",
        "detail": "unittest.test",
        "documentation": {}
    }
]